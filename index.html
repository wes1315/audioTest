<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>WAV/PCM Audio Capture</title>
</head>
<body>
  <h1>WAV/PCM Audio Capture</h1>
  <button id="startButton">Start Recording</button>
  <button id="stopButton" disabled>Stop Recording</button>

  <h2>Real-time recognizing results</h2>
  <div id="recognizingResults"></div>

  <h2>Final recognized results</h2>
  <div id="recognizedResults"></div>

  <h2>翻译结果 (Translated)</h2>
  <div id="translatedResults"></div>

<script>
    // global variables
    let audioContext;
    let scriptProcessor;
    let mediaStreamSource;
    let audioStream;
    let pcmBuffer = [];      // for temporarily storing data that is not yet 1 second
    let finalBlob;           // the wav file generated from the recording
    let socket = new WebSocket("ws://wesley:8765");
    
    const SAMPLE_RATE = 16000;  // 16 kHz sampling rate
    const CHUNK_SIZE = SAMPLE_RATE; // the number of samples for 1 second of audio
    const BUFFER_SIZE = 4096;   // the buffer size of the ScriptProcessorNode

    // WebSocket connection event
    socket.addEventListener("open", () => {
      console.log("Connected to WebSocket server");
    });

    // receive WebSocket messages, and process the Azure recognition results
    socket.addEventListener("message", (event) => {
      try {
        const data = JSON.parse(event.data);
        if (data.type === "recognizing") {
          // real-time recognition, overwrite the display
          document.getElementById("recognizingResults").innerText = data.result;
          console.log("real-time recognition result:", data.result);
        } else if (data.type === "recognized") {
          // final recognition, append to the display and auto scroll
          const recognizedDiv = document.getElementById("recognizedResults");
          recognizedDiv.innerHTML += data.result + "<br/>";
          recognizedDiv.scrollTop = recognizedDiv.scrollHeight;
          console.log("final recognized result:", data.result);
        } else if (data.type === "translated") {
          const translatedDiv = document.getElementById("translatedResults");
          translatedDiv.innerHTML += data.result + "<br/>";
          translatedDiv.scrollTop = translatedDiv.scrollHeight;
          console.log("translated result:", data.result);
        }
      } catch (error) {
        console.error("Error processing WebSocket message:", error);
      }
    });

    // reconnect method (optional)
    function connectWebSocket() {
      let ws = new WebSocket("ws://wesley:8765");
      ws.addEventListener("open", () => {
        console.log("Connected to WebSocket server");
      });
      ws.addEventListener("error", (error) => {
        console.error("WebSocket error:", error);
        setTimeout(() => {
          socket = connectWebSocket();
        }, 1000);
      });
      return ws;
    }

    // encode the pcm data as a wav blob (16-bit single channel)
    function encodeWAV(pcmData) {
      const buffer = new ArrayBuffer(44 + pcmData.length * 2);
      const view = new DataView(buffer);

      // WAV header
      writeString(view, 0, "RIFF"); // RIFF header
      view.setUint32(4, 36 + pcmData.length * 2, true); // File size
      writeString(view, 8, "WAVE"); // WAVE format
      writeString(view, 12, "fmt "); // fmt chunk
      view.setUint32(16, 16, true); // Subchunk1 size (16 for PCM)
      view.setUint16(20, 1, true);  // Audio format (1 表示 PCM)
      view.setUint16(22, 1, true);  // single channel
      view.setUint32(24, SAMPLE_RATE, true);  // sampling rate
      view.setUint32(28, SAMPLE_RATE * 2, true); // byte rate = sampling rate * 2 (2 bytes per sample)
      view.setUint16(32, 2, true);  // Block align = 2
      view.setUint16(34, 16, true); // Bits per sample
      writeString(view, 36, "data"); // data chunk
      view.setUint32(40, pcmData.length * 2, true); // Data size

      // write the pcm data, and convert to 16-bit integers
      let offset = 44;
      for (let i = 0; i < pcmData.length; i++, offset += 2) {
        const sample = Math.max(-1, Math.min(1, pcmData[i]));
        view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      }

      return new Blob([view], { type: "audio/wav" });
    }

    // auxiliary function: write a string to a DataView
    function writeString(view, offset, string) {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    }

    // send the wav blob to the WebSocket
    function scheduleSend(wavBlob) {
      setTimeout(async () => {
        try {
          if (socket.readyState === WebSocket.OPEN) {
            if (socket.bufferedAmount > 0) {
              console.log("WebSocket buffer is not empty");
            }
            const arrayBuffer = await wavBlob.arrayBuffer();
            socket.send(arrayBuffer);
            console.log("Sent 1 second of audio data at", Date.now());
          } else {
            console.error("WebSocket is not open");
          }
        } catch (error) {
          console.error("Error sending audio data:", error);
        }
      }, 0); // 异步调度
    }

    // 开始录制
    document.getElementById("startButton").addEventListener("click", async () => {
      try {
        // request microphone permission
        audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        // create an AudioContext, and specify the sampling rate
        audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });
        mediaStreamSource = audioContext.createMediaStreamSource(audioStream);

        // create a ScriptProcessorNode to capture the pcm data
        scriptProcessor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);
        scriptProcessor.onaudioprocess = (event) => {
          const inputData = event.inputBuffer.getChannelData(0);
          pcmBuffer.push(...inputData);

          // when the accumulated data reaches 1 second (CHUNK_SIZE samples), process it
          while (pcmBuffer.length >= CHUNK_SIZE) {
            const chunk = pcmBuffer.slice(0, CHUNK_SIZE);
            pcmBuffer = pcmBuffer.slice(CHUNK_SIZE);
            const wavBlob = encodeWAV(chunk);
            // send the 1 second data block
            scheduleSend(wavBlob);
          }
        };

        // establish the audio node connection: microphone -> processor -> AudioContext.destination (which helps activate the processor)
        mediaStreamSource.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination);

        console.log("Recording started");
        document.getElementById("startButton").disabled = true;
        document.getElementById("stopButton").disabled = false;
        
        // clear the previous recording data
        pcmBuffer = [];
      } catch (error) {
        console.error("Error accessing microphone:", error);
      }
    });

    // stop recording
    document.getElementById("stopButton").addEventListener("click", () => {
      if (scriptProcessor) {
        scriptProcessor.disconnect();
        scriptProcessor = null;
      }
      if (mediaStreamSource) {
        mediaStreamSource.disconnect();
        mediaStreamSource = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop());
        audioStream = null;
      }

      document.getElementById("startButton").disabled = false;
      document.getElementById("stopButton").disabled = true;
    });
  </script>
</body>
</html>
